{"title":"Research","markdown":{"yaml":{"title":"Research"},"headingText":"**Research Question 1:**","containsRefs":false,"markdown":"\n\n\nDo certain demographic groups have a higher or lower likelihood of defaulting on credit card payments?\n\n##### **Methodology:**\n\n**Regression Model**: This methodology involves using logistic regression to analyze the likelihood of defaulting on credit card payments based on demographic variables.\n\n**Data Preprocessing**:\n\nCategorical variables such as Gender, Education, Marriage, and default_payment_next_month are converted to factors to ensure compatibility with the regression model.\n\n**Exploratory Data Analysis (EDA)**:\n\nVisualizations are created to explore default payments across different demographic groups. This step helps to understand any patterns or trends in default behavior based on demographic characteristics.\n\nData exploration is performed using visualization to understand the distribution of default payments across different demographic groups. This is achieved through the creation of a bar plot ('plot1') using 'ggplot2' library.\n\n![](RQ%201.png){fig-align=\"center\"}\n\n**Model Training**:\n\n-   The dataset is split into training and testing sets using the 'createDataPartition()' function from the 'caret' library. This ensures that the model is trained on a subset of the data and evaluated on unseen data.\n-   A logistic regression model ('logit_model') is trained using the 'glm()' function. Logistic regression is a common method for binary classification tasks like predicting default payments.\n\n**Model Evaluation**:\n\n```{r, echo=FALSE, warning=FALSE, message=FALSE}\n\n# Load necessary libraries\nlibrary(readr)  # For data reading\nlibrary(dplyr)  # For data manipulation\nlibrary(ggplot2)  # For data visualization\nlibrary(caret)  # For machine learning models\n\n# Read the dataset\ncredit_data <- read.csv(\"C:\\\\Users\\\\nmadh\\\\OneDrive\\\\Desktop\\\\Final Project\\\\Final_output.csv\")\n\n# 1. Preprocess the data\n# Convert categorical variables to factors\ncredit_data$Gender <- as.factor(credit_data$Gender)\ncredit_data$Education <- as.factor(credit_data$Education)\ncredit_data$Marriage <- as.factor(credit_data$Marriage)\ncredit_data$default_payment_next_month <- as.factor(credit_data$default_payment_next_month)\n\n# 3. Train regression models\n# Split the data into training and testing sets\nset.seed(123)  # For reproducibility\ntrain_index <- createDataPartition(credit_data$default_payment_next_month, p = 0.7, list = FALSE)\ntrain_data <- credit_data[train_index, ]\ntest_data <- credit_data[-train_index, ]\n\n# Train logistic regression model\nlogit_model <- glm(default_payment_next_month ~ ., data = train_data, family = binomial)\n\n# 4. Evaluate model performance\n# Make predictions on the test set\npredictions <- predict(logit_model, newdata = test_data, type = \"response\")\npredicted_classes <- ifelse(predictions > 0.5, \"Default\", \"Not Default\")\n\n# Confusion matrix\nconfusionMatrix(table(predicted_classes, test_data$default_payment_next_month))\n```\n\n-   Predictions are made on the test set using the trained logistic regression model.\n-   Predicted classes are determined based on the probability thresholds (in this case, 0.5) using an ifelse statement.\n-   The confusion matrix is generated using the 'confusionMatrix()' function from the caret library. This matrix provides insights into the performance of the model, including metrics such as accuracy, sensitivity, specificity, etc.\n\n##### **Interpretation:**\n\nThe graph visually dissects default payment statuses across gender, education, and marital status combinations, offering valuable insights into default likelihoods across different demographic groups. It reveals trends suggesting that males may exhibit slightly higher default rates in certain contexts. Higher education levels, particularly advanced degrees, are associated with lower default rates, while lower education levels show higher default frequencies, especially among singles. Marital status nuances also influence default propensity, with married individuals, especially those with advanced degrees, displaying lower default rates. Certain combinations, like high school education and single status, appear more vulnerable to defaults. Overall, the graph provides detailed statistical representations enabling nuanced analysis of default trends within micro-demographics.\n\nIn conclusion, certain demographic groups indeed have a higher or lower likelihood of defaulting. Males in several panels, individuals with lower educational attainment, and singles are more prone to defaulting. Conversely, those with advanced degrees, particularly when married, show a lower likelihood of default. These insights are critical for credit risk management, allowing for tailored risk assessment strategies and targeted financial advice or product offerings to mitigate potential defaults.\n\n#### **Research Question 2:**\n\nWhich features are deemed most important in predicting credit default risk?\n\n##### **Methodology:**\n\n**Random Forest Model:** This methodology involves training a Random Forest model, extracting variable importance measures, sorting them, and then visualizing the feature importance to gain insights into which features are most important in predicting credit default risk.\n\n**Setting Seed for Reproducibility**:\n\nSetting a seed (set.seed(1234)) ensures that the random number generation during model training is reproducible. This means that if you rerun the code with the same seed, you should get the same results.\n\n**Training the Random Forest Model**:\n\nThe randomForest function is used to train a Random Forest model. This function requires specifying the formula for the model, where default_payment_next_month \\~ . indicates that default_payment_next_month is the target variable, and . indicates that all other variables in the dataset are used as predictors.\n\n**Variable Importance Measures**: The importance function is applied to the trained Random Forest model (rf_model) to obtain variable importance measures. Variable importance provides insights into which predictor variables are most influential in predicting the target variable (credit default risk). The measure used here is the Mean Decrease in Gini Index. The obtained variable importance measures are sorted in descending order using the order function. Sorting helps in identifying the most important predictor variables at the top of the list.\n\n```{r, echo=FALSE, warning=FALSE, message=FALSE}\n\ndata <- read.csv(\"C:\\\\Users\\\\nmadh\\\\OneDrive\\\\Desktop\\\\Final Project\\\\Final_output.csv\")\n\ndata$Gender<- factor(data$Gender)\ndata$Education <- factor(data$Education)\ndata$Marriage <- factor(data$Marriage)\ndata$Repay_Sept <- factor(data$Repay_Sept)\ndata$Repay_Aug <- factor(data$Repay_Aug)\ndata$Repay_Jul <- factor(data$Repay_Jul)\ndata$Repay_Jun <- factor(data$Repay_Jun)\ndata$Repay_May <- factor(data$Repay_May)\ndata$Repay_Apr <- factor(data$Repay_Apr)\ndata$default_payment_next_month <- factor(data$default_payment_next_month)\n\n# Load required libraries\nlibrary(randomForest)\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Train the Random Forest model\nrf_model <- randomForest(default_payment_next_month ~ ., data = data)\n\n# Get variable importance measures\nimportance <- importance(rf_model)\n\n# Sort variable importance measures by importance\nvar_importance <- importance[order(-importance[, 1]), ]\n\n# Display variable importance measures\nprint(var_importance)\n```\n\nThe sorted variable importance measures are printed to the console like above, allowing you to see the importance values of each predictor variable.\n\n![](images/clipboard-1601144281.png)\n\n**Feature Importance**:\n\n\\- After obtaining the variable importance measures, the code further processes them to extract the importance values and corresponding feature names.\n\n\\- The importance values are sorted in descending order, and the corresponding feature names are ordered accordingly.\n\n\\- This plot provides a graphical representation of each feature's importance in predicting credit default risk, making it easier to interpret and identify the most influential features.\n\n**Conclusion:** The insights gained from this analysis can inform credit risk assessment processes and aid in the development of more accurate predictive models for credit default risk.\n\n##### **Interpretation:**\n\nThe bar chart illustrates the importance of various features in predicting credit default risk, with repayment status in September (Repay_Sept) emerging as the most critical predictor. This highlights the significance of recent financial behaviors in forecasting default likelihood. The model also emphasizes the importance of bill amounts and repayment statuses in other months, underscoring the value of consistent financial habits over time. Conversely, demographic features like Gender, Education, and Marriage have lesser impact, suggesting that behavioral factors outweigh demographic variables in predicting default risk. These insights enable financial institutions to refine their risk assessment models, tailor monitoring strategies, and develop targeted interventions for customers exhibiting risky financial behaviors.\n\nIn summary, the Random Forest model identifies repayment statuses, particularly the most recent ones, and bill amounts as the most informative features for predicting credit default risk. This aligns with the notion that recent and consistent financial behaviors provide a clearer picture of a customer's financial health than static demographic data. This insight is invaluable for developing more effective risk assessment tools and customer management strategies in the financial sector.\n\n#### **Research Question 3:**\n\nIdentify the distinct patterns of credit limit utilization among users.\n\n##### **Methodology:**\n\n**Clustering:** The methodology to answer the above question involves several key steps for clustering users based on their patterns of credit limit utilization.\n\n**Data Selection:**\n\nRelevant columns related to credit limit utilization are chosen from the dataset. These columns typically include variables representing the repayment behavior across different months.\n\n**Data Standardization:**\n\nThe selected data is standardized to ensure that all variables have a mean of 0 and a standard deviation of 1. This process, known as z-score normalization, is essential for clustering algorithms like K-means, as it prevents variables with larger scales from dominating the analysis.\n\n**Determining Optimal Number of Clusters:**\n\nThe code employs the \"elbow method\" to determine the optimal number of clusters. This method involves calculating the within-cluster sum of squares for different numbers of clusters and plotting these values. The point where adding more clusters does not significantly decrease the within-cluster sum of squares indicates the optimal number of clusters.\n\n**Performing K-means Clustering:**\n\nOnce the optimal number of clusters is determined, the K-means algorithm is applied to the standardized data. K-means is an iterative algorithm that partitions the data into the specified number of clusters by minimizing the within-cluster variance. Assigning Cluster Labels: Each user is assigned a cluster label based on the results of the K-means clustering algorithm. These labels represent the cluster to which each user belongs.\n\n**Adding Cluster Labels to the Dataset:**\n\nThe cluster labels are added back to the original dataset, allowing for further analysis and interpretation of the clustering results.\n\n**Visualizing Cluster Centers:**\n\nThe code performs Principal Component Analysis (PCA) on the cluster centers to reduce the dimensionality of the data. PCA transforms the high-dimensional cluster centers into a lower-dimensional space while preserving the most important information. The reduced-dimensional data is then visualized to gain insights into the distinct patterns of credit limit utilization among different clusters.\n\nOverall, this methodology enables the identification of meaningful patterns in credit limit utilization behavior and provides insights into how users can be grouped based on these patterns.\n\n##### **Interpretation:**\n\nThe images and corresponding data processing steps describe the utilization of K-means clustering to identify distinct patterns of credit limit utilization among users based on multiple credit-related behaviors.\n\n**Elbow Plot:**\n\nThe elbow plot is used to determine the optimal number of clusters by showing the within-groups sum of squares (WSS) for different numbers of clusters. This metric measures the compactness of the clusters, with lower values generally indicating better clustering (less spread within the clusters).\n\n![](images/clipboard-1802327773.png)\n\n**Key Observations from the Elbow Plot:**\n\nA sharp decline in WSS from 2 to 3 clusters suggests significant improvements in cluster compactness with the addition of clusters in this range. The curve begins to flatten around 3 clusters, suggesting diminishing returns on improvement in cluster compactness with additional clusters. This is a typical indicator used to select the optimal number of clusters; in this case, it suggests that 3 clusters may be suitable. Second Image: PCA Plot of Clusters The second image displays a scatter plot of the first two principal components of the standardized data, colored by the assigned cluster labels (1, 2, 3). This visualization helps in interpreting the spatial distribution of clusters in the reduced dimensionality space.\n\n**PCA Plot of Clusters:**\n\nThe below graph displays a scatter plot of the first two principal components of the standardized data, colored by the assigned cluster labels (1, 2, 3). This visualization helps in interpreting the spatial distribution of clusters in the reduced dimensionality space.\n\n![](images/clipboard-3389825363.png)\n\n**Key Observations from PCA Plot:**\n\nCluster 1 (Pink): This cluster is distinctly positioned towards the positive side of the second principal component. This might indicate a specific pattern of credit utilization or repayment behavior that separates this group from the others.\n\nCluster 2 (Green): Occupying the middle ground in the plot, this cluster shows moderate values on both principal components. This could suggest an average credit behavior compared to the extremes exhibited by the other two clusters.\n\nCluster 3 (Black): Concentrated primarily in the center and negative side of the first principal component, this cluster could represent users with lower credit utilization or more consistent repayment behaviors compared to the others. Identifying Distinct Patterns of Credit Limit Utilization Based on the clustering analysis, three distinct patterns of credit utilization among users can be identified:\n\nHigh Utilization/Erratic Repayment: Likely represented by Cluster 1, these users might be utilizing their credit limits to a high extent and might have more volatile repayment histories. Moderate Utilization/Stable Repayment: Cluster 2's positioning suggests users in this group have moderate credit utilization and more stable repayment patterns. Low Utilization/Consistent Repayment: Cluster 3 could be indicative of users who use their credit limits conservatively and maintain consistent repayment behaviors.\n\nConclusion:\\\nThese clusters help in understanding the different strategies and financial behaviors of credit card users, which can be crucial for credit risk management and marketing strategies tailored to different consumer segments. By recognizing these distinct patterns, financial institutions can better cater to the varying needs and risks associated with different groups of users.\n\n##### \n","srcMarkdownNoYaml":"\n\n#### **Research Question 1:**\n\nDo certain demographic groups have a higher or lower likelihood of defaulting on credit card payments?\n\n##### **Methodology:**\n\n**Regression Model**: This methodology involves using logistic regression to analyze the likelihood of defaulting on credit card payments based on demographic variables.\n\n**Data Preprocessing**:\n\nCategorical variables such as Gender, Education, Marriage, and default_payment_next_month are converted to factors to ensure compatibility with the regression model.\n\n**Exploratory Data Analysis (EDA)**:\n\nVisualizations are created to explore default payments across different demographic groups. This step helps to understand any patterns or trends in default behavior based on demographic characteristics.\n\nData exploration is performed using visualization to understand the distribution of default payments across different demographic groups. This is achieved through the creation of a bar plot ('plot1') using 'ggplot2' library.\n\n![](RQ%201.png){fig-align=\"center\"}\n\n**Model Training**:\n\n-   The dataset is split into training and testing sets using the 'createDataPartition()' function from the 'caret' library. This ensures that the model is trained on a subset of the data and evaluated on unseen data.\n-   A logistic regression model ('logit_model') is trained using the 'glm()' function. Logistic regression is a common method for binary classification tasks like predicting default payments.\n\n**Model Evaluation**:\n\n```{r, echo=FALSE, warning=FALSE, message=FALSE}\n\n# Load necessary libraries\nlibrary(readr)  # For data reading\nlibrary(dplyr)  # For data manipulation\nlibrary(ggplot2)  # For data visualization\nlibrary(caret)  # For machine learning models\n\n# Read the dataset\ncredit_data <- read.csv(\"C:\\\\Users\\\\nmadh\\\\OneDrive\\\\Desktop\\\\Final Project\\\\Final_output.csv\")\n\n# 1. Preprocess the data\n# Convert categorical variables to factors\ncredit_data$Gender <- as.factor(credit_data$Gender)\ncredit_data$Education <- as.factor(credit_data$Education)\ncredit_data$Marriage <- as.factor(credit_data$Marriage)\ncredit_data$default_payment_next_month <- as.factor(credit_data$default_payment_next_month)\n\n# 3. Train regression models\n# Split the data into training and testing sets\nset.seed(123)  # For reproducibility\ntrain_index <- createDataPartition(credit_data$default_payment_next_month, p = 0.7, list = FALSE)\ntrain_data <- credit_data[train_index, ]\ntest_data <- credit_data[-train_index, ]\n\n# Train logistic regression model\nlogit_model <- glm(default_payment_next_month ~ ., data = train_data, family = binomial)\n\n# 4. Evaluate model performance\n# Make predictions on the test set\npredictions <- predict(logit_model, newdata = test_data, type = \"response\")\npredicted_classes <- ifelse(predictions > 0.5, \"Default\", \"Not Default\")\n\n# Confusion matrix\nconfusionMatrix(table(predicted_classes, test_data$default_payment_next_month))\n```\n\n-   Predictions are made on the test set using the trained logistic regression model.\n-   Predicted classes are determined based on the probability thresholds (in this case, 0.5) using an ifelse statement.\n-   The confusion matrix is generated using the 'confusionMatrix()' function from the caret library. This matrix provides insights into the performance of the model, including metrics such as accuracy, sensitivity, specificity, etc.\n\n##### **Interpretation:**\n\nThe graph visually dissects default payment statuses across gender, education, and marital status combinations, offering valuable insights into default likelihoods across different demographic groups. It reveals trends suggesting that males may exhibit slightly higher default rates in certain contexts. Higher education levels, particularly advanced degrees, are associated with lower default rates, while lower education levels show higher default frequencies, especially among singles. Marital status nuances also influence default propensity, with married individuals, especially those with advanced degrees, displaying lower default rates. Certain combinations, like high school education and single status, appear more vulnerable to defaults. Overall, the graph provides detailed statistical representations enabling nuanced analysis of default trends within micro-demographics.\n\nIn conclusion, certain demographic groups indeed have a higher or lower likelihood of defaulting. Males in several panels, individuals with lower educational attainment, and singles are more prone to defaulting. Conversely, those with advanced degrees, particularly when married, show a lower likelihood of default. These insights are critical for credit risk management, allowing for tailored risk assessment strategies and targeted financial advice or product offerings to mitigate potential defaults.\n\n#### **Research Question 2:**\n\nWhich features are deemed most important in predicting credit default risk?\n\n##### **Methodology:**\n\n**Random Forest Model:** This methodology involves training a Random Forest model, extracting variable importance measures, sorting them, and then visualizing the feature importance to gain insights into which features are most important in predicting credit default risk.\n\n**Setting Seed for Reproducibility**:\n\nSetting a seed (set.seed(1234)) ensures that the random number generation during model training is reproducible. This means that if you rerun the code with the same seed, you should get the same results.\n\n**Training the Random Forest Model**:\n\nThe randomForest function is used to train a Random Forest model. This function requires specifying the formula for the model, where default_payment_next_month \\~ . indicates that default_payment_next_month is the target variable, and . indicates that all other variables in the dataset are used as predictors.\n\n**Variable Importance Measures**: The importance function is applied to the trained Random Forest model (rf_model) to obtain variable importance measures. Variable importance provides insights into which predictor variables are most influential in predicting the target variable (credit default risk). The measure used here is the Mean Decrease in Gini Index. The obtained variable importance measures are sorted in descending order using the order function. Sorting helps in identifying the most important predictor variables at the top of the list.\n\n```{r, echo=FALSE, warning=FALSE, message=FALSE}\n\ndata <- read.csv(\"C:\\\\Users\\\\nmadh\\\\OneDrive\\\\Desktop\\\\Final Project\\\\Final_output.csv\")\n\ndata$Gender<- factor(data$Gender)\ndata$Education <- factor(data$Education)\ndata$Marriage <- factor(data$Marriage)\ndata$Repay_Sept <- factor(data$Repay_Sept)\ndata$Repay_Aug <- factor(data$Repay_Aug)\ndata$Repay_Jul <- factor(data$Repay_Jul)\ndata$Repay_Jun <- factor(data$Repay_Jun)\ndata$Repay_May <- factor(data$Repay_May)\ndata$Repay_Apr <- factor(data$Repay_Apr)\ndata$default_payment_next_month <- factor(data$default_payment_next_month)\n\n# Load required libraries\nlibrary(randomForest)\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Train the Random Forest model\nrf_model <- randomForest(default_payment_next_month ~ ., data = data)\n\n# Get variable importance measures\nimportance <- importance(rf_model)\n\n# Sort variable importance measures by importance\nvar_importance <- importance[order(-importance[, 1]), ]\n\n# Display variable importance measures\nprint(var_importance)\n```\n\nThe sorted variable importance measures are printed to the console like above, allowing you to see the importance values of each predictor variable.\n\n![](images/clipboard-1601144281.png)\n\n**Feature Importance**:\n\n\\- After obtaining the variable importance measures, the code further processes them to extract the importance values and corresponding feature names.\n\n\\- The importance values are sorted in descending order, and the corresponding feature names are ordered accordingly.\n\n\\- This plot provides a graphical representation of each feature's importance in predicting credit default risk, making it easier to interpret and identify the most influential features.\n\n**Conclusion:** The insights gained from this analysis can inform credit risk assessment processes and aid in the development of more accurate predictive models for credit default risk.\n\n##### **Interpretation:**\n\nThe bar chart illustrates the importance of various features in predicting credit default risk, with repayment status in September (Repay_Sept) emerging as the most critical predictor. This highlights the significance of recent financial behaviors in forecasting default likelihood. The model also emphasizes the importance of bill amounts and repayment statuses in other months, underscoring the value of consistent financial habits over time. Conversely, demographic features like Gender, Education, and Marriage have lesser impact, suggesting that behavioral factors outweigh demographic variables in predicting default risk. These insights enable financial institutions to refine their risk assessment models, tailor monitoring strategies, and develop targeted interventions for customers exhibiting risky financial behaviors.\n\nIn summary, the Random Forest model identifies repayment statuses, particularly the most recent ones, and bill amounts as the most informative features for predicting credit default risk. This aligns with the notion that recent and consistent financial behaviors provide a clearer picture of a customer's financial health than static demographic data. This insight is invaluable for developing more effective risk assessment tools and customer management strategies in the financial sector.\n\n#### **Research Question 3:**\n\nIdentify the distinct patterns of credit limit utilization among users.\n\n##### **Methodology:**\n\n**Clustering:** The methodology to answer the above question involves several key steps for clustering users based on their patterns of credit limit utilization.\n\n**Data Selection:**\n\nRelevant columns related to credit limit utilization are chosen from the dataset. These columns typically include variables representing the repayment behavior across different months.\n\n**Data Standardization:**\n\nThe selected data is standardized to ensure that all variables have a mean of 0 and a standard deviation of 1. This process, known as z-score normalization, is essential for clustering algorithms like K-means, as it prevents variables with larger scales from dominating the analysis.\n\n**Determining Optimal Number of Clusters:**\n\nThe code employs the \"elbow method\" to determine the optimal number of clusters. This method involves calculating the within-cluster sum of squares for different numbers of clusters and plotting these values. The point where adding more clusters does not significantly decrease the within-cluster sum of squares indicates the optimal number of clusters.\n\n**Performing K-means Clustering:**\n\nOnce the optimal number of clusters is determined, the K-means algorithm is applied to the standardized data. K-means is an iterative algorithm that partitions the data into the specified number of clusters by minimizing the within-cluster variance. Assigning Cluster Labels: Each user is assigned a cluster label based on the results of the K-means clustering algorithm. These labels represent the cluster to which each user belongs.\n\n**Adding Cluster Labels to the Dataset:**\n\nThe cluster labels are added back to the original dataset, allowing for further analysis and interpretation of the clustering results.\n\n**Visualizing Cluster Centers:**\n\nThe code performs Principal Component Analysis (PCA) on the cluster centers to reduce the dimensionality of the data. PCA transforms the high-dimensional cluster centers into a lower-dimensional space while preserving the most important information. The reduced-dimensional data is then visualized to gain insights into the distinct patterns of credit limit utilization among different clusters.\n\nOverall, this methodology enables the identification of meaningful patterns in credit limit utilization behavior and provides insights into how users can be grouped based on these patterns.\n\n##### **Interpretation:**\n\nThe images and corresponding data processing steps describe the utilization of K-means clustering to identify distinct patterns of credit limit utilization among users based on multiple credit-related behaviors.\n\n**Elbow Plot:**\n\nThe elbow plot is used to determine the optimal number of clusters by showing the within-groups sum of squares (WSS) for different numbers of clusters. This metric measures the compactness of the clusters, with lower values generally indicating better clustering (less spread within the clusters).\n\n![](images/clipboard-1802327773.png)\n\n**Key Observations from the Elbow Plot:**\n\nA sharp decline in WSS from 2 to 3 clusters suggests significant improvements in cluster compactness with the addition of clusters in this range. The curve begins to flatten around 3 clusters, suggesting diminishing returns on improvement in cluster compactness with additional clusters. This is a typical indicator used to select the optimal number of clusters; in this case, it suggests that 3 clusters may be suitable. Second Image: PCA Plot of Clusters The second image displays a scatter plot of the first two principal components of the standardized data, colored by the assigned cluster labels (1, 2, 3). This visualization helps in interpreting the spatial distribution of clusters in the reduced dimensionality space.\n\n**PCA Plot of Clusters:**\n\nThe below graph displays a scatter plot of the first two principal components of the standardized data, colored by the assigned cluster labels (1, 2, 3). This visualization helps in interpreting the spatial distribution of clusters in the reduced dimensionality space.\n\n![](images/clipboard-3389825363.png)\n\n**Key Observations from PCA Plot:**\n\nCluster 1 (Pink): This cluster is distinctly positioned towards the positive side of the second principal component. This might indicate a specific pattern of credit utilization or repayment behavior that separates this group from the others.\n\nCluster 2 (Green): Occupying the middle ground in the plot, this cluster shows moderate values on both principal components. This could suggest an average credit behavior compared to the extremes exhibited by the other two clusters.\n\nCluster 3 (Black): Concentrated primarily in the center and negative side of the first principal component, this cluster could represent users with lower credit utilization or more consistent repayment behaviors compared to the others. Identifying Distinct Patterns of Credit Limit Utilization Based on the clustering analysis, three distinct patterns of credit utilization among users can be identified:\n\nHigh Utilization/Erratic Repayment: Likely represented by Cluster 1, these users might be utilizing their credit limits to a high extent and might have more volatile repayment histories. Moderate Utilization/Stable Repayment: Cluster 2's positioning suggests users in this group have moderate credit utilization and more stable repayment patterns. Low Utilization/Consistent Repayment: Cluster 3 could be indicative of users who use their credit limits conservatively and maintain consistent repayment behaviors.\n\nConclusion:\\\nThese clusters help in understanding the different strategies and financial behaviors of credit card users, which can be crucial for credit risk management and marketing strategies tailored to different consumer segments. By recognizing these distinct patterns, financial institutions can better cater to the varying needs and risks associated with different groups of users.\n\n##### \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"Research.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Research"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}